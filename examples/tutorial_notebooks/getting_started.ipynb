{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with MultiVae\n",
    "\n",
    "In this tutorial we detail a simple example on how to train and evaluate a model with MultiVae."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a toy dataset\n",
    "\n",
    "As a toy example here (and to allow fast training), we load the MnistLabels dataset from the library.\n",
    "The first modality is the image and the second \"modality\" is the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.data.datasets.mnist_labels import MnistLabels\n",
    "\n",
    "DATA_PATH = './data' # Set the path where to download the data\n",
    "dataset = MnistLabels(DATA_PATH, 'test',download=True) # Set download to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all datasets in MultiVae, the ```__getitem__``` function returns a ```~pythae.data.datasets.DatasetOutput``` class which contains a field `data` with all modalities (here 'images' and 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modalities in that dataset are named ['images', 'labels']\n",
      "The text attribute for this image is tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize one sample\n",
    "import matplotlib.pyplot as plt\n",
    "sample = dataset[0]\n",
    "\n",
    "# A sample contains a field 'data' with the data for both modalities (here images and labels) and eventual additional fields (labels, masks..)\n",
    "print(f'The modalities in that dataset are named {list(sample.data.keys())}')\n",
    "\n",
    "plt.imshow(sample.data['images'][0], cmap='gray')\n",
    "print(f'The text attribute for this image is {sample.data[\"labels\"]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model \n",
    "\n",
    "Now that we have our dataset, we can import the model of our choice. To have a list of available models, you can look at the documentation [here](https://multivae.readthedocs.io/en/latest/models/multivae.models.html). In the documentation, you also have a description of each model's hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model of your choice\n",
    "from multivae.models import MVTCAE, MVTCAEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration\n",
    "\n",
    "model_config = MVTCAEConfig(\n",
    "    n_modalities=2,\n",
    "    latent_dim=20,\n",
    "    input_dims= {'images' : (1,28,28), 'labels' : (1,10)},\n",
    "    decoders_dist= {'images' : 'normal', 'labels' : 'categorical'}, # Distributions to use for the decoders. It defines the reconstruction loss.\n",
    "    \n",
    "    alpha=2./3.,# hyperparameters specific to this model\n",
    "    beta=2.5,\n",
    "    \n",
    "    uses_likelihood_rescaling=True, # rescale the reconstruction loss for better results\n",
    "    rescale_factors=dict(images=1, labels=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the default Multi-Layer-Perceptron architectures. Of course for more complex use-cases, you might want to provide your own architectures through the encoders/ decoders argument of the model. We will see how to do that in the next section of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "# If no encoders/ decoders architectures are specified, default MLPs are used\n",
    "model = MVTCAE(model_config = model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelOutput([('loss', tensor(626.6713, grad_fn=<DivBackward0>)),\n",
       "             ('loss_sum', tensor(6266.7134, grad_fn=<AddBackward0>)),\n",
       "             ('metrics',\n",
       "              {'joint_divergence': tensor(19.5532, grad_fn=<MulBackward0>),\n",
       "               'images': tensor(8185.5713, grad_fn=<SumBackward0>),\n",
       "               'labels': tensor(1141.2080, grad_fn=<SumBackward0>),\n",
       "               'kld_images': tensor(19.9876, grad_fn=<SumBackward0>),\n",
       "               'kld_labels': tensor(19.0915, grad_fn=<SumBackward0>)})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that everything works\n",
    "model(dataset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward function of the model computes the mean loss on the batch as well as some metrics. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom architectures\n",
    "\n",
    "You can use custom architectures instead of the default ones. Your architectures simply need to inherit from the [BaseEncoder](https://pythae.readthedocs.io/en/latest/models/nn/pythae_base_nn.html#pythae.models.nn.BaseEncoder) class and the [BaseDecoder](https://pythae.readthedocs.io/en/latest/models/nn/pythae_base_nn.html#pythae.models.nn.BaseDecoder) class. \n",
    "Below we define two simple convolutional architecture that can be used to encode the MNIST images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.base import BaseEncoder, BaseDecoder, ModelOutput\n",
    "import torch\n",
    "\n",
    "# The custom encoder must be an instance of the BaseEncoder class\n",
    "class ImageEncoder(BaseEncoder):\n",
    "    \n",
    "    \"A simple custom convolutional architecture to use on MNIST images.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.latent_dim = 20 \n",
    "        \n",
    "        self.conv_net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Flatten(start_dim=1)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.embedding_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * 3 * 32, self.latent_dim)\n",
    "        )\n",
    "        self.covariance_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * 3 * 32, self.latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.conv_net(x)\n",
    "        \n",
    "        #### The output of the variational encoder must be a ModelOutput instance \n",
    "        #### with an embedding field and log_covariance field\n",
    "        \n",
    "        return ModelOutput(\n",
    "            embedding = self.embedding_layer(h),\n",
    "            log_covariance = self.covariance_layer(h)\n",
    "        )\n",
    "    \n",
    "# The custom decoder must be an instance of the BaseDecoder class\n",
    "class ImageDecoder(BaseDecoder):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 3*3*32),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Unflatten(dim=1,\n",
    "            unflattened_size=(32, 3, 3))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv_net = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(32, 16, 3, \n",
    "            stride=2, output_padding=0),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.linear(x)\n",
    "        h = self.conv_net(h)\n",
    "        x = torch.sigmoid(h)\n",
    "        \n",
    "        #### The output must be a ModelOutput instance with a 'reconstruction' field\n",
    "        return ModelOutput(reconstruction = x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate a model with custom architectures provided. Note that for the labels, we use simple MLPs from the MultiVae library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.nn.default_architectures import Encoder_VAE_MLP, Decoder_AE_MLP, BaseAEConfig\n",
    "\n",
    "label_encoder = Encoder_VAE_MLP(BaseAEConfig(input_dim = (10,), latent_dim=20))\n",
    "label_decoder = Decoder_AE_MLP(BaseAEConfig(input_dim=(10,), latent_dim=20))\n",
    "\n",
    "model_custom = MVTCAE(model_config=model_config,\n",
    "               encoders = dict(images = ImageEncoder(), labels=label_encoder),\n",
    "               decoders=dict(images = ImageDecoder(), labels=label_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelOutput([('loss', tensor(1315.3213, grad_fn=<DivBackward0>)),\n",
       "             ('loss_sum', tensor(13153.2129, grad_fn=<AddBackward0>)),\n",
       "             ('metrics',\n",
       "              {'joint_divergence': tensor(19.1381, grad_fn=<MulBackward0>),\n",
       "               'images': tensor(8097.8481, grad_fn=<SumBackward0>),\n",
       "               'labels': tensor(11559.4805, grad_fn=<SumBackward0>),\n",
       "               'kld_images': tensor(19.4139, grad_fn=<SumBackward0>),\n",
       "               'kld_labels': tensor(19.4402, grad_fn=<SumBackward0>)})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that everything works\n",
    "model_custom(dataset[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In MultiVae, training can be easily performed using the MultiVae's trainers. Below we define a simple trainer and use it for training our model. \n",
    "Many arguments can be passed on to `BaseTrainerConfig`: check out the [documentation](https://multivae.readthedocs.io/en/latest/trainers/base.html#multivae.trainers.BaseTrainer) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Setting the optimizer with learning rate 0.01\n",
      "Created dummy_output_dir/MVTCAE_training_2025-03-14_15-46-44. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from multivae.trainers import BaseTrainer, BaseTrainerConfig\n",
    "\n",
    "#Define the training configuration\n",
    "trainer_config = BaseTrainerConfig(\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-2, \n",
    "    optimizer_cls='Adam',\n",
    "    output_dir='dummy_output_dir'\n",
    "    \n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = BaseTrainer(\n",
    "    model=model,\n",
    "    training_config=trainer_config,\n",
    "    train_dataset=dataset\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training params:\n",
      " - max_epochs: 30\n",
      " - per_device_train_batch_size: 64\n",
      " - per_device_eval_batch_size: 64\n",
      " - checkpoint saving every: None\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Scheduler: None\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/30:  95%|█████████▍| 149/157 [00:01<00:00, 107.49batch/s]New best model on train saved!\n",
      "Training of epoch 1/30: 100%|██████████| 157/157 [00:01<00:00, 108.39batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7224\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/30: 100%|██████████| 157/157 [00:01<00:00, 113.95batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7241\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/30:  98%|█████████▊| 154/157 [00:01<00:00, 104.60batch/s]New best model on train saved!\n",
      "Training of epoch 3/30: 100%|██████████| 157/157 [00:01<00:00, 108.36batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.6961\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/30: 100%|██████████| 157/157 [00:01<00:00, 107.85batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7744\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/30: 100%|██████████| 157/157 [00:01<00:00, 107.60batch/s]New best model on train saved!\n",
      "Training of epoch 5/30: 100%|██████████| 157/157 [00:01<00:00, 104.19batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.6128\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/30: 100%|██████████| 157/157 [00:01<00:00, 103.43batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.6455\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/30: 100%|██████████| 157/157 [00:01<00:00, 108.17batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.0614\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/30: 100%|██████████| 157/157 [00:01<00:00, 108.39batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8275\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/30:  94%|█████████▎| 147/157 [00:01<00:00, 101.57batch/s]New best model on train saved!\n",
      "Training of epoch 9/30: 100%|██████████| 157/157 [00:01<00:00, 103.47batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.5967\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/30: 100%|██████████| 157/157 [00:01<00:00, 108.10batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.6352\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/30: 100%|██████████| 157/157 [00:01<00:00, 116.97batch/s]New best model on train saved!\n",
      "Training of epoch 11/30: 100%|██████████| 157/157 [00:01<00:00, 112.49batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.4928\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/30: 100%|██████████| 157/157 [00:01<00:00, 110.16batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8132\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/30: 100%|██████████| 157/157 [00:01<00:00, 110.59batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.6179\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/30: 100%|██████████| 157/157 [00:01<00:00, 109.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7388\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/30: 100%|██████████| 157/157 [00:01<00:00, 108.24batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7164\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/30: 100%|██████████| 157/157 [00:01<00:00, 113.21batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8276\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/30: 100%|██████████| 157/157 [00:01<00:00, 109.28batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.9291\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/30: 100%|██████████| 157/157 [00:01<00:00, 112.35batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8729\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/30: 100%|██████████| 157/157 [00:01<00:00, 109.80batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8367\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/30: 100%|██████████| 157/157 [00:01<00:00, 110.39batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7792\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/30: 100%|██████████| 157/157 [00:01<00:00, 111.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.9331\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/30: 100%|██████████| 157/157 [00:01<00:00, 109.52batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.6987\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/30: 100%|██████████| 157/157 [00:01<00:00, 111.25batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8997\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/30: 100%|██████████| 157/157 [00:01<00:00, 110.90batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.0421\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/30: 100%|██████████| 157/157 [00:01<00:00, 104.96batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7561\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/30: 100%|██████████| 157/157 [00:01<00:00, 106.61batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.5788\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/30: 100%|██████████| 157/157 [00:01<00:00, 113.62batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8261\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/30: 100%|██████████| 157/157 [00:01<00:00, 107.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8679\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/30: 100%|██████████| 157/157 [00:01<00:00, 103.09batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.7651\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/30: 100%|██████████| 157/157 [00:01<00:00, 110.14batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 556.8779\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/MVTCAE_training_2025-03-14_15-46-44/final_model\n"
     ]
    }
   ],
   "source": [
    "# Now we train:\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can reload the best model saved by the trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.auto_model import AutoModel\n",
    "\n",
    "best_model = AutoModel.load_from_folder(f'{trainer.training_dir}/final_model') # Copy the path to final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `predict` function of the model to generate some samples, conditioning on a specific label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images generated by conditioning on label =  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = best_model.eval()\n",
    "\n",
    "# images from label : show a few samples \n",
    "predictions = best_model.predict(\n",
    "    inputs=dataset[0],\n",
    "    cond_mod='labels', # The labels will be used as the conditioning modality\n",
    "    N = 4, # We generate 4 samples\n",
    ")\n",
    "\n",
    "# Plot\n",
    "print(\"Images generated by conditioning on label = \", dataset[0].data['labels'])\n",
    "fig, ax = plt.subplots(1,4)\n",
    "for i in range(4):\n",
    "    ax[i].imshow(predictions['images'][i][0][0].detach(), cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing metrics\n",
    "\n",
    "To further evaluate your model, you can compute some metrics like likelihoods, coherence, clustering metrics, reconstruction metrics, or additional visualization. You can check out the [documentation](https://multivae.readthedocs.io/en/latest/metrics/multivae.metrics.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:11<00:00,  6.83it/s]\n",
      "Mean Joint likelihood : tensor(752.5036)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelOutput([('joint_likelihood', tensor(752.5036))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multivae.metrics.likelihoods import LikelihoodsEvaluator, LikelihoodsEvaluatorConfig\n",
    "\n",
    "ll_config = LikelihoodsEvaluatorConfig(\n",
    "    batch_size=128,\n",
    "    num_samples=100)\n",
    "\n",
    "ll = LikelihoodsEvaluator(\n",
    "    best_model,dataset,eval_config=ll_config\n",
    ")\n",
    "\n",
    "ll.eval() #might take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
