{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with MultiVae\n",
    "\n",
    "In this tutorial we detail a simple example on how to train and evaluate a model with MultiVae."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a toy dataset\n",
    "\n",
    "As a toy example here (and to allow fast training), we load the MnistLabels dataset from the library.\n",
    "The first modality is the image and the second \"modality\" is the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.data.datasets.mnist_labels import MnistLabels\n",
    "\n",
    "DATA_PATH = './data' # Set the path where to download the data\n",
    "dataset = MnistLabels(DATA_PATH, 'test',download=True) # Set download to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all datasets in MultiVae, the ```__getitem__``` function returns a ```~pythae.data.datasets.DatasetOutput``` class which contains a field `data` with all modalities (here 'images' and 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modalities in that dataset are named ['images', 'labels']\n",
      "The text attribute for this image is tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize one sample\n",
    "import matplotlib.pyplot as plt\n",
    "sample = dataset[0]\n",
    "\n",
    "# A sample contains a field 'data' with the data for both modalities (here images and labels) and eventual additional fields (labels, masks..)\n",
    "print(f'The modalities in that dataset are named {list(sample.data.keys())}')\n",
    "\n",
    "plt.imshow(sample.data['images'][0], cmap='gray')\n",
    "print(f'The text attribute for this image is {sample.data[\"labels\"]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model \n",
    "\n",
    "Now that we have our dataset, we can import the model of our choice. To have a list of available models, you can look at the documentation [here](https://multivae.readthedocs.io/en/latest/models/multivae.models.html). In the documentation, you also have a description of each model's hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model of your choice\n",
    "from multivae.models import MVTCAE, MVTCAEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration\n",
    "\n",
    "model_config = MVTCAEConfig(\n",
    "    n_modalities=2,\n",
    "    latent_dim=20,\n",
    "    input_dims= {'images' : (1,28,28), 'labels' : (1,10)},\n",
    "    decoders_dist= {'images' : 'normal', 'labels' : 'categorical'}, # Distributions to use for the decoders. It defines the reconstruction loss.\n",
    "    \n",
    "    alpha=2./3.,\n",
    "    beta=2.5,\n",
    "    \n",
    "    uses_likelihood_rescaling=True,\n",
    "    rescale_factors=dict(images=1, labels=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the default Multi-Layer-Perceptron architectures. Of course for more complex use-cases, you might want to provide your own architectures through the encoders/ decoders argument of the model. We will see how to do that in the next section of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "# If no encoders/ decoders architectures are specified, default MLPs are used\n",
    "model = MVTCAE(model_config = model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelOutput([('loss', tensor(628.3367, grad_fn=<DivBackward0>)),\n",
       "             ('loss_sum', tensor(6283.3667, grad_fn=<AddBackward0>)),\n",
       "             ('metrics',\n",
       "              {'joint_divergence': tensor(19.6089, grad_fn=<MulBackward0>),\n",
       "               'images': tensor(8192.8711, grad_fn=<SumBackward0>),\n",
       "               'labels': tensor(1159.0118, grad_fn=<SumBackward0>),\n",
       "               'kld_images': tensor(19.7253, grad_fn=<SumBackward0>),\n",
       "               'kld_labels': tensor(19.1992, grad_fn=<SumBackward0>)})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that everything works\n",
    "model(dataset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward function of the model computes the mean loss on the batch as well as some metrics. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom architectures\n",
    "\n",
    "You can use custom architectures instead of the default ones. Your architectures simply need to inherit from the [BaseEncoder](https://pythae.readthedocs.io/en/latest/models/nn/pythae_base_nn.html#pythae.models.nn.BaseEncoder) class and the [BaseDecoder](https://pythae.readthedocs.io/en/latest/models/nn/pythae_base_nn.html#pythae.models.nn.BaseDecoder) class. \n",
    "Below we define two simple convolutional architecture that can be used to encode the MNIST images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.base import BaseEncoder, BaseDecoder, ModelOutput\n",
    "import torch\n",
    "\n",
    "# The custom encoder must be an instance of the BaseEncoder class\n",
    "class ImageEncoder(BaseEncoder):\n",
    "    \n",
    "    \"A simple custom convolutional architecture to use on MNIST images.\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.latent_dim = 20 \n",
    "        \n",
    "        self.conv_net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Flatten(start_dim=1)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.embedding_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * 3 * 32, self.latent_dim)\n",
    "        )\n",
    "        self.covariance_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * 3 * 32, self.latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.conv_net(x)\n",
    "        \n",
    "        #### The output of the variational encoder must be a ModelOutput instance \n",
    "        #### with an embedding field and log_covariance field\n",
    "        \n",
    "        return ModelOutput(\n",
    "            embedding = self.embedding_layer(h),\n",
    "            log_covariance = self.covariance_layer(h)\n",
    "        )\n",
    "    \n",
    "# The custom decoder must be an instance of the BaseDecoder class\n",
    "class ImageDecoder(BaseDecoder):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 3*3*32),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Unflatten(dim=1,\n",
    "            unflattened_size=(32, 3, 3))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv_net = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(32, 16, 3, \n",
    "            stride=2, output_padding=0),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
    "            padding=1, output_padding=1),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.linear(x)\n",
    "        h = self.conv_net(h)\n",
    "        x = torch.sigmoid(h)\n",
    "        \n",
    "        #### The output must be a ModelOutput instance with a 'reconstruction' field\n",
    "        return ModelOutput(reconstruction = x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate a model with custom architectures provided. Note that for the labels, we use simple MLPs from the MultiVae library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.nn.default_architectures import Encoder_VAE_MLP, Decoder_AE_MLP, BaseAEConfig\n",
    "\n",
    "label_encoder = Encoder_VAE_MLP(BaseAEConfig(input_dim = (10,), latent_dim=20))\n",
    "label_decoder = Decoder_AE_MLP(BaseAEConfig(input_dim=(10,), latent_dim=20))\n",
    "\n",
    "model_custom = MVTCAE(model_config=model_config,\n",
    "               encoders = dict(images = ImageEncoder(), labels=label_encoder),\n",
    "               decoders=dict(images = ImageDecoder(), labels=label_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelOutput([('loss', tensor(1307.8213, grad_fn=<DivBackward0>)),\n",
       "             ('loss_sum', tensor(13078.2129, grad_fn=<AddBackward0>)),\n",
       "             ('metrics',\n",
       "              {'joint_divergence': tensor(19.3067, grad_fn=<MulBackward0>),\n",
       "               'images': tensor(8025.1890, grad_fn=<SumBackward0>),\n",
       "               'labels': tensor(11519.3818, grad_fn=<SumBackward0>),\n",
       "               'kld_images': tensor(19.3244, grad_fn=<SumBackward0>),\n",
       "               'kld_labels': tensor(19.5676, grad_fn=<SumBackward0>)})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that everything works\n",
    "model_custom(dataset[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In MultiVae, training can be easily performed using the MultiVae's trainers. Below we define a simple trainer and use it for training our model. \n",
    "Many arguments can be passed on to `BaseTrainerConfig`: check out the [documentation](https://multivae.readthedocs.io/en/latest/trainers/base.html#multivae.trainers.BaseTrainer) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Setting the optimizer with learning rate 0.01\n",
      "Created dummy_output_dir/MVTCAE_training_2025-02-24_10-22-27. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from multivae.trainers import BaseTrainer, BaseTrainerConfig\n",
    "\n",
    "#Define the training configuration\n",
    "trainer_config = BaseTrainerConfig(\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-2, \n",
    "    optimizer_cls='Adam',\n",
    "    output_dir='dummy_output_dir'\n",
    "    \n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = BaseTrainer(\n",
    "    model=model,\n",
    "    training_config=trainer_config,\n",
    "    train_dataset=dataset\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training params:\n",
      " - max_epochs: 10\n",
      " - per_device_train_batch_size: 64\n",
      " - per_device_eval_batch_size: 64\n",
      " - checkpoint saving every: None\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Scheduler: None\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/10:  95%|█████████▍| 149/157 [00:01<00:00, 101.61batch/s]New best model on train saved!\n",
      "Training of epoch 1/10: 100%|██████████| 157/157 [00:01<00:00, 98.24batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 559.0608\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/10:  94%|█████████▎| 147/157 [00:01<00:00, 99.72batch/s]New best model on train saved!\n",
      "Training of epoch 2/10: 100%|██████████| 157/157 [00:01<00:00, 98.42batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 558.526\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/10:  97%|█████████▋| 152/157 [00:01<00:00, 102.39batch/s]New best model on train saved!\n",
      "Training of epoch 3/10: 100%|██████████| 157/157 [00:01<00:00, 100.33batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 558.2096\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/10:  98%|█████████▊| 154/157 [00:01<00:00, 101.86batch/s]New best model on train saved!\n",
      "Training of epoch 4/10: 100%|██████████| 157/157 [00:01<00:00, 101.06batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.8896\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/10:  97%|█████████▋| 153/157 [00:01<00:00, 102.04batch/s]New best model on train saved!\n",
      "Training of epoch 5/10: 100%|██████████| 157/157 [00:01<00:00, 100.91batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.5799\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/10: 100%|██████████| 157/157 [00:01<00:00, 99.60batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.6723\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/10:  97%|█████████▋| 152/157 [00:01<00:00, 100.93batch/s]New best model on train saved!\n",
      "Training of epoch 7/10: 100%|██████████| 157/157 [00:01<00:00, 98.92batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.3774\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/10:  96%|█████████▌| 150/157 [00:01<00:00, 100.80batch/s]New best model on train saved!\n",
      "Training of epoch 8/10: 100%|██████████| 157/157 [00:01<00:00, 98.97batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.3164\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/10: 100%|██████████| 157/157 [00:01<00:00, 110.40batch/s]New best model on train saved!\n",
      "Training of epoch 9/10: 100%|██████████| 157/157 [00:01<00:00, 104.77batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.2829\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/10:  93%|█████████▎| 146/157 [00:01<00:00, 110.19batch/s]New best model on train saved!\n",
      "Training of epoch 10/10: 100%|██████████| 157/157 [00:01<00:00, 107.66batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.0842\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/MVTCAE_training_2025-02-24_10-22-27/final_model\n"
     ]
    }
   ],
   "source": [
    "# Now we train:\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can reload the best model saved by the trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.auto_model import AutoModel\n",
    "\n",
    "best_model = AutoModel.load_from_folder(f'{trainer.training_dir}/final_model') # Copy the path to final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `predict` function of the model to generate some samples, conditioning on a specific label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images generated by conditioning on label =  tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = best_model.eval()\n",
    "\n",
    "# images from label : show a few samples \n",
    "predictions = best_model.predict(\n",
    "    inputs=dataset[0],\n",
    "    cond_mod='labels', # The labels will be used as the conditioning modality\n",
    "    N = 4, # We generate 4 samples\n",
    ")\n",
    "\n",
    "# Plot\n",
    "print(\"Images generated by conditioning on label = \", dataset[0].data['labels'])\n",
    "fig, ax = plt.subplots(1,4)\n",
    "for i in range(4):\n",
    "    ax[i].imshow(predictions['images'][i][0][0].detach(), cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing metrics\n",
    "\n",
    "To further evaluate your model, you can compute some metrics like likelihoods, coherence, clustering metrics, reconstruction metrics, or additional visualization. You can check out the [documentation](https://multivae.readthedocs.io/en/latest/metrics/multivae.metrics.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:15<00:00,  5.19it/s]\n",
      "Joint likelihood : tensor(752.7989)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelOutput([('joint_likelihood', tensor(752.7989))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multivae.metrics.likelihoods import LikelihoodsEvaluator, LikelihoodsEvaluatorConfig\n",
    "\n",
    "ll_config = LikelihoodsEvaluatorConfig(\n",
    "    batch_size=128,\n",
    "    num_samples=100)\n",
    "\n",
    "ll = LikelihoodsEvaluator(\n",
    "    best_model,dataset,eval_config=ll_config\n",
    ")\n",
    "\n",
    "ll.eval() #might take some time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
