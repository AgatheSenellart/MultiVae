{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore your model with Wandb\n",
    "\n",
    "In this tutorial we detail a simple example on how to monitor your training / and validation using WandB. \n",
    "\n",
    "First, if you don't have wandb installed yet, follow the installation instructions:\n",
    "\n",
    "## Install wandb\n",
    "1. Install wandb ```$ pip install wandb```\n",
    "2. Create a wandb account [online](https://wandb.ai/)\n",
    "3. Once you are logged in, go to this [page](https://wandb.ai/authorize) and copy the API key. \n",
    "4. In your terminal, enter ```$ wandb login``` and then copy your API key when prompted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your dataset, model \n",
    "\n",
    "First define the dataset and model you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.data.datasets.mnist_labels import MnistLabels\n",
    "\n",
    "# Import the dataset\n",
    "DATA_PATH = \"./data\"  # Set the path where to download the data\n",
    "dataset = MnistLabels(DATA_PATH, \"test\", download=True)  # Set download to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model of your choice\n",
    "from multivae.models import MVTCAE, MVTCAEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration\n",
    "\n",
    "model_config = MVTCAEConfig(\n",
    "    n_modalities=2,\n",
    "    latent_dim=20,\n",
    "    input_dims={\"images\": (1, 28, 28), \"labels\": (1, 10)},\n",
    "    decoders_dist={\n",
    "        \"images\": \"normal\",\n",
    "        \"labels\": \"categorical\",\n",
    "    },  # Distributions to use for the decoders. It defines the reconstruction loss.\n",
    "    alpha=2.0 / 3.0,  # hyperparameters specific to this model\n",
    "    beta=2.5,\n",
    "    uses_likelihood_rescaling=True,  # rescale the reconstruction loss for better results\n",
    "    rescale_factors=dict(images=1, labels=50),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "# If no encoders/ decoders architectures are specified, default MLPs are used\n",
    "model = MVTCAE(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a wandb callback and pass it to your trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asenella/miniconda3/envs/multivaenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masenellart\u001b[0m (\u001b[33mmultimodal_vaes\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/asenella/dev/MultiVae/examples/tutorial_notebooks/wandb/run-20250314_160516-bxoxeod2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2' target=\"_blank\">cinnamon-bun-5</a></strong> to <a href='https://wandb.ai/multimodal_vaes/wandb_notebook' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/multimodal_vaes/wandb_notebook' target=\"_blank\">https://wandb.ai/multimodal_vaes/wandb_notebook</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2' target=\"_blank\">https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! No eval dataset provided ! -> keeping best model on train.\n",
      "\n",
      "Model passed sanity check !\n",
      "Ready for training.\n",
      "\n",
      "Setting the optimizer with learning rate 0.01\n",
      "Created dummy_output_dir/MVTCAE_training_2025-03-14_16-05-17. \n",
      "Training config, checkpoints and final model will be saved here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from multivae.trainers import BaseTrainer, BaseTrainerConfig\n",
    "from multivae.trainers.base.callbacks import WandbCallback\n",
    "\n",
    "# Define the training configuration\n",
    "trainer_config = BaseTrainerConfig(\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-2,\n",
    "    optimizer_cls=\"Adam\",\n",
    "    output_dir=\"dummy_output_dir\",\n",
    "    steps_predict=5,  # !! set this argument to log images of generation to Wandb every 5 epochs !!\n",
    ")\n",
    "\n",
    "# !Define your wandb callback!\n",
    "wandb_cb = WandbCallback()\n",
    "# Pass the training config and model config\n",
    "wandb_cb.setup(\n",
    "    training_config=trainer_config,\n",
    "    model_config=model_config,\n",
    "    project_name=\"wandb_notebook\",\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = BaseTrainer(\n",
    "    model=model,\n",
    "    training_config=trainer_config,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[wandb_cb],  ## !!! Pass the callback to the trainer !!!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training params:\n",
      " - max_epochs: 30\n",
      " - per_device_train_batch_size: 64\n",
      " - per_device_eval_batch_size: 64\n",
      " - checkpoint saving every: None\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Scheduler: None\n",
      "\n",
      "Successfully launched training !\n",
      "\n",
      "Training of epoch 1/30:  96%|█████████▌| 151/157 [00:01<00:00, 139.66batch/s]New best model on train saved!\n",
      "/home/asenella/dev/MultiVae/src/multivae/data/datasets/mnist_labels.py:93: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  fig.tight_layout()\n",
      "Training of epoch 1/30: 100%|██████████| 157/157 [00:03<00:00, 42.53batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 564.5122\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 2/30:  97%|█████████▋| 152/157 [00:01<00:00, 133.95batch/s]New best model on train saved!\n",
      "Training of epoch 2/30: 100%|██████████| 157/157 [00:01<00:00, 130.94batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 559.4471\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 3/30:  94%|█████████▍| 148/157 [00:01<00:00, 128.92batch/s]New best model on train saved!\n",
      "Training of epoch 3/30: 100%|██████████| 157/157 [00:01<00:00, 127.52batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 558.8545\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 4/30:  93%|█████████▎| 146/157 [00:01<00:00, 121.23batch/s]New best model on train saved!\n",
      "Training of epoch 4/30: 100%|██████████| 157/157 [00:01<00:00, 120.61batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 558.1606\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 5/30:  98%|█████████▊| 154/157 [00:01<00:00, 114.09batch/s]New best model on train saved!\n",
      "Training of epoch 5/30: 100%|██████████| 157/157 [00:03<00:00, 48.46batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.9769\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 6/30:  97%|█████████▋| 153/157 [00:01<00:00, 133.68batch/s]New best model on train saved!\n",
      "Training of epoch 6/30: 100%|██████████| 157/157 [00:01<00:00, 131.53batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.6687\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 7/30: 100%|██████████| 157/157 [00:01<00:00, 131.02batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.6781\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 8/30:  92%|█████████▏| 145/157 [00:01<00:00, 137.55batch/s]New best model on train saved!\n",
      "Training of epoch 8/30: 100%|██████████| 157/157 [00:01<00:00, 135.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.5197\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 9/30:  94%|█████████▍| 148/157 [00:01<00:00, 106.44batch/s]New best model on train saved!\n",
      "Training of epoch 9/30: 100%|██████████| 157/157 [00:01<00:00, 111.02batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.4495\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 10/30: 100%|██████████| 157/157 [00:03<00:00, 43.48batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.5341\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 11/30:  97%|█████████▋| 153/157 [00:01<00:00, 131.32batch/s]New best model on train saved!\n",
      "Training of epoch 11/30: 100%|██████████| 157/157 [00:01<00:00, 129.28batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.294\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 12/30: 100%|██████████| 157/157 [00:01<00:00, 128.65batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.3899\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 13/30: 100%|██████████| 157/157 [00:01<00:00, 128.98batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.3938\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 14/30:  97%|█████████▋| 153/157 [00:01<00:00, 132.05batch/s]New best model on train saved!\n",
      "Training of epoch 14/30: 100%|██████████| 157/157 [00:01<00:00, 132.13batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.2696\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 15/30:  96%|█████████▌| 151/157 [00:01<00:00, 125.08batch/s]New best model on train saved!\n",
      "Training of epoch 15/30: 100%|██████████| 157/157 [00:03<00:00, 50.14batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.236\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 16/30:  98%|█████████▊| 154/157 [00:01<00:00, 137.70batch/s]New best model on train saved!\n",
      "Training of epoch 16/30: 100%|██████████| 157/157 [00:01<00:00, 136.44batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.1873\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 17/30: 100%|██████████| 157/157 [00:01<00:00, 132.76batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.2739\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 18/30: 100%|██████████| 157/157 [00:01<00:00, 131.29batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.2474\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 19/30: 100%|██████████| 157/157 [00:01<00:00, 134.63batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.3718\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 20/30:  95%|█████████▍| 149/157 [00:01<00:00, 134.77batch/s]New best model on train saved!\n",
      "Training of epoch 20/30: 100%|██████████| 157/157 [00:02<00:00, 56.48batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.175\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 21/30:  97%|█████████▋| 152/157 [00:01<00:00, 136.21batch/s]New best model on train saved!\n",
      "Training of epoch 21/30: 100%|██████████| 157/157 [00:01<00:00, 131.40batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.1434\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 22/30:  96%|█████████▌| 150/157 [00:01<00:00, 134.03batch/s]New best model on train saved!\n",
      "Training of epoch 22/30: 100%|██████████| 157/157 [00:01<00:00, 130.07batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.1067\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 23/30: 100%|██████████| 157/157 [00:01<00:00, 132.01batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.2693\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 24/30: 100%|██████████| 157/157 [00:01<00:00, 127.91batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.2707\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 25/30: 100%|██████████| 157/157 [00:03<00:00, 49.17batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.9016\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 26/30: 100%|██████████| 157/157 [00:01<00:00, 129.87batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.7998\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 27/30: 100%|██████████| 157/157 [00:01<00:00, 123.93batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.1822\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 28/30: 100%|██████████| 157/157 [00:01<00:00, 130.16batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.6005\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 29/30: 100%|██████████| 157/157 [00:01<00:00, 130.38batch/s]\n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.4288\n",
      "--------------------------------------------------------------------------\n",
      "Training of epoch 30/30: 100%|██████████| 157/157 [00:03<00:00, 47.81batch/s] \n",
      "--------------------------------------------------------------------------\n",
      "Train loss: 557.5418\n",
      "--------------------------------------------------------------------------\n",
      "Training ended!\n",
      "Saved final model in dummy_output_dir/MVTCAE_training_2025-03-14_16-05-17/final_model\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/images</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/joint_divergence</td><td>▆█▇▅▅▄▃▄▄▃▃▃▃▃▄▃▂▃▃▃▂▃▂▂▃▃▁▃▃▃</td></tr><tr><td>train/kld_images</td><td>▆▅▄▃▃▂▃▂▂▂▁▂▃▂▂▂▃▂▄▁▂▂▂▂█▇▄▅▄▅</td></tr><tr><td>train/kld_labels</td><td>█▇▆▅▅▅▄▄▄▄▄▄▃▅▄▃▄▅▂▆▄▅▃▆▁▃▃▄▄▄</td></tr><tr><td>train/labels</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch_loss</td><td>557.54183</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/images</td><td>47707.08203</td></tr><tr><td>train/joint_divergence</td><td>276.43442</td></tr><tr><td>train/kld_images</td><td>260.86557</td></tr><tr><td>train/kld_labels</td><td>116.92857</td></tr><tr><td>train/labels</td><td>4743.45508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cinnamon-bun-5</strong> at: <a href='https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2' target=\"_blank\">https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2</a><br> View project at: <a href='https://wandb.ai/multimodal_vaes/wandb_notebook' target=\"_blank\">https://wandb.ai/multimodal_vaes/wandb_notebook</a><br>Synced 5 W&B file(s), 21 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250314_160516-bxoxeod2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we train:\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics and log to WandB\n",
    "\n",
    "When computing metrics afterwards, you can log the results to the same wandb path. \n",
    "If you reload your model in a different script and don't know where to find the wandb path, check out this [link](https://multivae.readthedocs.io/en/latest/metrics/info_wandb.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivae.models.auto_model import AutoModel\n",
    "\n",
    "# reload the best model\n",
    "best_model = AutoModel.load_from_folder(\n",
    "    f\"{trainer.training_dir}/final_model\"\n",
    ")  # Copy the path to final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/asenella/dev/MultiVae/examples/tutorial_notebooks/wandb/run-20250314_161112-bxoxeod2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2' target=\"_blank\">cinnamon-bun-5</a></strong> to <a href='https://wandb.ai/multimodal_vaes/wandb_notebook' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/multimodal_vaes/wandb_notebook' target=\"_blank\">https://wandb.ai/multimodal_vaes/wandb_notebook</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2' target=\"_blank\">https://wandb.ai/multimodal_vaes/wandb_notebook/runs/bxoxeod2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:10<00:00,  7.53it/s]\n",
      "Mean Joint likelihood : tensor(752.7980)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelOutput([('joint_likelihood', tensor(752.7980))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multivae.metrics.likelihoods import (\n",
    "    LikelihoodsEvaluator,\n",
    "    LikelihoodsEvaluatorConfig,\n",
    ")\n",
    "\n",
    "# here we get the path from the wandb_cb object that we created earlier\n",
    "wandb_path = wandb_cb.run.path\n",
    "\n",
    "ll_config = LikelihoodsEvaluatorConfig(\n",
    "    batch_size=128,\n",
    "    num_samples=100,\n",
    "    wandb_path=wandb_path,  # ! pass the wandb_path here !\n",
    ")\n",
    "\n",
    "ll = LikelihoodsEvaluator(best_model, dataset, eval_config=ll_config)\n",
    "\n",
    "ll.eval()  # might take some time\n",
    "ll.finish()  # to finish the wandb run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
