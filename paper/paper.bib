@inproceedings{wu:2018,
	title = {Multimodal {Generative} {Models} for {Scalable} {Weakly}-{Supervised} {Learning}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/1102a326d5f7c9e04fc3c89d0ede88c9-Abstract.html},
	abstract = {Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations.Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations---edge detection, colorization,  segmentation---as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.},
	urldate = {2022-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Mike and Goodman, Noah},
	year = {2018},
	file = {Full Text PDF:/Users/agathe/Zotero/storage/MWRQ9XZ3/Wu et Goodman - 2018 - Multimodal Generative Models for Scalable Weakly-S.pdf:application/pdf},
}

@article{suzuki:2016,
	title = {Joint {Multimodal} {Learning} with {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1611.01891},
	abstract = {We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.},
	urldate = {2022-04-15},
	journal = {arXiv:1611.01891 [cs, stat]},
	author = {Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01891},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/agathe/Zotero/storage/8FFNR92S/Suzuki et al. - 2016 - Joint Multimodal Learning with Deep Generative Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/agathe/Zotero/storage/A5F38V4M/1611.html:text/html},
}

@article{papamakarios_masked_nodate,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the ﬂexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing ﬂow suitable for density estimation, which we call Masked Autoregressive Flow. This type of ﬂow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	language = {en},
	author = {Papamakarios, George and Murray, Iain and Pavlakou, Theo},
	pages = {10},
	file = {Papamakarios et al. - Masked Autoregressive Flow for Density Estimation.pdf:/Users/agathe/Zotero/storage/SMKG4KW7/Papamakarios et al. - Masked Autoregressive Flow for Density Estimation.pdf:application/pdf},
}

@article{palumbo_mmvae_2023,
	title = {{MMVAE}+: {ENHANCING} {THE} {GENERATIVE} {QUALITY} {OF} {MULTIMODAL} {VAES} {WITHOUT} {COMPROMISES}},
	abstract = {Multimodal VAEs have recently gained attention as efficient models for weaklysupervised generative learning with multiple modalities. However, all existing variants of multimodal VAEs are affected by a non-trivial trade-off between generative quality and generative coherence. In particular mixture-based models achieve good coherence only at the expense of sample diversity and a resulting lack of generative quality. We present a novel variant of the mixture-of-experts multimodal variational autoencoder that improves its generative quality, while maintaining high semantic coherence. We model shared and modality-specific information in separate latent subspaces, proposing an objective that overcomes certain dependencies on hyperparameters that arise for existing approaches with the same latent space structure. Compared to these existing approaches, we show increased robustness with respect to changes in the design of the latent space, in terms of the capacity allocated to modality-specific subspaces. We show that our model achieves both good generative coherence and high generative quality in challenging experiments, including more complex multimodal datasets than those used in previous works.},
	language = {en},
	author = {Palumbo, Emanuele and Daunhawer, Imant and Vogt, Julia E},
	year = {2023},
	file = {Palumbo et al. - 2023 - MMVAE+ ENHANCING THE GENERATIVE QUALITY OF MULTIM.pdf:/Users/agathe/Zotero/storage/4BB3EVIZ/Palumbo et al. - 2023 - MMVAE+ ENHANCING THE GENERATIVE QUALITY OF MULTIM.pdf:application/pdf},
}


@article{vedantam:2018,
	title = {Generative {Models} of {Visually} {Grounded} {Imagination}},
	url = {http://arxiv.org/abs/1705.10762},
	abstract = {It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially speciﬁed (abstract) concepts in a principled and efﬁcient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C’s). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of Suzuki et al. (2017) and the BiVCCA method of Wang et al. (2016b)) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset (Liu et al., 2015).},
	language = {en},
	urldate = {2022-05-12},
	journal = {arXiv:1705.10762 [cs, stat]},
	author = {Vedantam, Ramakrishna and Fischer, Ian and Huang, Jonathan and Murphy, Kevin},
	month = nov,
	year = {2018},
	note = {arXiv: 1705.10762},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, TELBO},
	file = {Vedantam et al. - 2018 - Generative Models of Visually Grounded Imagination.pdf:/Users/agathe/Zotero/storage/S4CBSAZR/Vedantam et al. - 2018 - Generative Models of Visually Grounded Imagination.pdf:application/pdf},
}
@inproceedings{pythae,
	title = {Pythae: {Unifying} {Generative} {Autoencoders} in {Python} - {A} {Benchmarking} {Use} {Case}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/872f0e04ef95be7970d9a9d74b198fdf-Paper-Datasets_and_Benchmarks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chadebec, Clément and Vincent, Louis and Allassonniere, Stephanie},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {21575--21589},
}
@misc{kingma,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efﬁcient by ﬁtting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.},
	language = {en},
	urldate = {2022-07-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:/Users/agathe/Zotero/storage/9YS7YFSU/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf},
}

@article{sutter:2021,
	title = {Generalized {Multimodal} {ELBO}},
	abstract = {This work proposes a new, generalized ELBO formulation for multimodal data that overcomes limitations of existing self-supervised generative models approximating an ELBO and demonstrates the advantage of the proposed method compared to state-of-the-art models in selfsupervised, generative learning tasks. Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in selfsupervised, generative learning tasks.},
	journal = {ICLR},
	author = {Sutter, Thomas M. and Daunhawer, Imant and Vogt, Julia E.},
	year = {2021},
	file = {Full Text PDF:/Users/agathe/Zotero/storage/BW7EBFA7/Sutter et al. - 2021 - Generalized Multimodal ELBO.pdf:application/pdf},
}
@article{shi:2019,
	title = {Variational {Mixture}-of-{Experts} {Autoencoders} for {Multi}-{Modal} {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1911.03393},
	abstract = {Learning generative models that span multiple data modalities, such as vision and language, is often motivated by the desire to learn more useful, generalisable representations that faithfully capture common underlying factors between the modalities. In this work, we characterise successful learning of such models as the fulfillment of four criteria: i) implicit latent decomposition into shared and private subspaces, ii) coherent joint generation over all modalities, iii) coherent cross-generation across individual modalities, and iv) improved model learning for individual modalities through multi-modal integration. Here, we propose a mixture-of-experts multimodal variational autoencoder (MMVAE) to learn generative models on different sets of modalities, including a challenging image-language dataset, and demonstrate its ability to satisfy all four criteria, both qualitatively and quantitatively.},
	urldate = {2022-04-20},
	journal = {arXiv:1911.03393 [cs, stat]},
	author = {Shi, Yuge and Siddharth, N. and Paige, Brooks and Torr, Philip H. S.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.03393},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/agathe/Zotero/storage/N7UJJ3EC/Shi et al. - 2019 - Variational Mixture-of-Experts Autoencoders for Mu.pdf:application/pdf;arXiv.org Snapshot:/Users/agathe/Zotero/storage/DSPZBBWC/1911.html:text/html},
}

@inproceedings{poklukar_geometric_2022,
	title = {Geometric {Multimodal} {Contrastive} {Representation} {Learning}},
	url = {https://proceedings.mlr.press/v162/poklukar22a.html},
	abstract = {Learning representations of multimodal data that are both informative and robust to missing modalities at test time remains a challenging problem due to the inherent heterogeneity of data obtained from different channels. To address it, we present a novel Geometric Multimodal Contrastive (GMC) representation learning method consisting of two main components: i) a two-level architecture consisting of modality-specific base encoders, allowing to process an arbitrary number of modalities to an intermediate representation of fixed dimensionality, and a shared projection head, mapping the intermediate representations to a latent representation space; ii) a multimodal contrastive loss function that encourages the geometric alignment of the learned representations. We experimentally demonstrate that GMC representations are semantically rich and achieve state-of-the-art performance with missing modality information on three different learning problems including prediction and reinforcement learning tasks.},
	language = {en},
	urldate = {2023-01-05},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Poklukar, Petra and Vasco, Miguel and Yin, Hang and Melo, Francisco S. and Paiva, Ana and Kragic, Danica},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {17782--17800},
	file = {Full Text PDF:/Users/agathe/Zotero/storage/354KCMYY/Poklukar et al. - 2022 - Geometric Multimodal Contrastive Representation Le.pdf:application/pdf},
}
@misc{wang_deep_2017,
	title = {Deep {Variational} {Canonical} {Correlation} {Analysis}},
	url = {http://arxiv.org/abs/1610.03454},
	doi = {10.48550/arXiv.1610.03454},
	abstract = {We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks. We derive variational lower bounds of the data likelihood by parameterizing the posterior probability of the latent variables from the view that is available at test time. We also propose a variant of VCCA called VCCA-private that can, in addition to the "common variables" underlying both views, extract the "private variables" within each view, and disentangles the shared and private information for multi-view data without hard supervision. Experimental results on real-world datasets show that our methods are competitive across domains.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Wang, Weiran and Yan, Xinchen and Lee, Honglak and Livescu, Karen},
	month = feb,
	year = {2017},
	note = {arXiv:1610.03454 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/agathe/Zotero/storage/JRM6NYA2/Wang et al. - 2017 - Deep Variational Canonical Correlation Analysis.pdf:application/pdf;arXiv.org Snapshot:/Users/agathe/Zotero/storage/FZVF7VGH/1610.html:text/html},
}
@article{vasco2022leveraging,
  title={Leveraging hierarchy in multimodal generative models for effective cross-modality inference},
  author={Vasco, Miguel and Yin, Hang and Melo, Francisco S and Paiva, Ana},
  journal={Neural Networks},
  volume={146},
  pages={238--255},
  year={2022},
  publisher={Elsevier}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}
@article{hwang2021multi,
  title={Multi-view representation learning via total correlation objective},
  author={Hwang, HyeongJoo and Kim, Geon-Hyeong and Hong, Seunghoon and Kim, Kee-Eung},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12194--12207},
  year={2021}
}

@article{senellart:2023,
  title={Improving Multimodal Joint Variational Autoencoders through Normalizing Flows and Correlation Analysis},
  author={Senellart, Agathe and Chadebec, Cl{\'e}ment and Allassonni{\`e}re, St{\'e}phanie},
  journal={arXiv preprint arXiv:2305.11832},
  year={2023}
}

@article{Aguila2023, 
doi = {10.21105/joss.05093}, 
url = {https://doi.org/10.21105/joss.05093},
 year = {2023}, publisher = {The Open Journal},
  volume = {8}, number = {85},
   pages = {5093}, 
   author = {Ana Lawry Aguila and Alejandra Jayme and Nina Montaña-Brown and Vincent Heuveline and Andre Altmann}, 
   title = {Multi-view-AE: A Python package for multi-view autoencoder models}, journal = {Journal of Open Source Software}
}

@online{huggingface,
  author = {Hugging Face},
  title = {Hugging Face Hub},
  year = 2023,
  url = {https://huggingface.co/docs/hub/index}
}

@article{suzuki_survey_2022,
	title = {A survey of multimodal deep generative models},
	volume = {36},
	issn = {0169-1864, 1568-5535},
	url = {http://arxiv.org/abs/2207.02127},
	doi = {10.1080/01691864.2022.2035253},
	abstract = {Multimodal learning is a framework for building models that make predictions based on diﬀerent types of modalities. Important challenges in multimodal learning are the inference of shared representations from arbitrary modalities and cross-modal generation via these representations; however, achieving this requires taking the heterogeneous nature of multimodal data into account. In recent years, deep generative models, i.e., generative models in which distributions are parameterized by deep neural networks, have attracted much attention, especially variational autoencoders, which are suitable for accomplishing the above challenges because they can consider heterogeneity and infer good representations of data. Therefore, various multimodal generative models based on variational autoencoders, called multimodal deep generative models, have been proposed in recent years. In this paper, we provide a categorized survey of studies on multimodal deep generative models.},
	language = {en},
	number = {5-6},
	urldate = {2022-12-09},
	journal = {Advanced Robotics},
	author = {Suzuki, Masahiro and Matsuo, Yutaka},
	month = mar,
	year = {2022},
	note = {arXiv:2207.02127 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {261--278},
	file = {Suzuki and Matsuo - 2022 - A survey of multimodal deep generative models.pdf:/Users/agathe/Zotero/storage/BNFUZAWU/Suzuki and Matsuo - 2022 - A survey of multimodal deep generative models.pdf:application/pdf},
}

@article{tian:2019,
	title = {Latent {Translation}: {Crossing} {Modalities} by {Bridging} {Generative} {Models}},
	shorttitle = {Latent {Translation}},
	abstract = {This work takes inspiration from neural machine translation, and cast the challenging problem of cross-modal domain transfer as unsupervised translation between the latent spaces of pretrained deep generative models as a post-hoc interface between two existing models to solve a new task. End-to-end optimization has achieved state-of-the-art performance on many specific problems, but there is no straight-forward way to combine pretrained models for new problems. Here, we explore improving modularity by learning a post-hoc interface between two existing models to solve a new task. Specifically, we take inspiration from neural machine translation, and cast the challenging problem of cross-modal domain transfer as unsupervised translation between the latent spaces of pretrained deep generative models. By abstracting away the data representation, we demonstrate that it is possible to transfer across different modalities (e.g., image-to-audio) and even different types of generative models (e.g., VAE-to-GAN). We compare to state-of-the-art techniques and find that a straight-forward variational autoencoder is able to best bridge the two generative models through learning a shared latent space. We can further impose supervised alignment of attributes in both domains with a classifier in the shared latent space. Through qualitative and quantitative evaluations, we demonstrate that locality and semantic alignment are preserved through the transfer process, as indicated by high transfer accuracies and smooth interpolations within a class. Finally, we show this modular structure speeds up training of new interface models by several orders of magnitude by decoupling it from expensive retraining of base generative models.},
	journal = {ArXiv},
	author = {Tian, Yingtao and Engel, Jesse},
	year = {2019},
	file = {Full Text PDF:/Users/agathe/Zotero/storage/LAW4V8IV/Tian et Engel - 2019 - Latent Translation Crossing Modalities by Bridgin.pdf:application/pdf},
}

@misc{dvcca,
	title = {Deep {Variational} {Canonical} {Correlation} {Analysis}},
	url = {http://arxiv.org/abs/1610.03454},
	doi = {10.48550/arXiv.1610.03454},
	abstract = {We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks. We derive variational lower bounds of the data likelihood by parameterizing the posterior probability of the latent variables from the view that is available at test time. We also propose a variant of VCCA called VCCA-private that can, in addition to the "common variables" underlying both views, extract the "private variables" within each view, and disentangles the shared and private information for multi-view data without hard supervision. Experimental results on real-world datasets show that our methods are competitive across domains.},
	urldate = {2023-01-18},
	publisher = {arXiv},
	author = {Wang, Weiran and Yan, Xinchen and Lee, Honglak and Livescu, Karen},
	month = feb,
	year = {2017},
	note = {arXiv:1610.03454 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/agathe/Zotero/storage/JRM6NYA2/Wang et al. - 2017 - Deep Variational Canonical Correlation Analysis.pdf:application/pdf;arXiv.org Snapshot:/Users/agathe/Zotero/storage/FZVF7VGH/1610.html:text/html},
}

@inbook{Dorent_2023,
   title={Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations},
   ISBN={9783031439995},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-031-43999-5_43},
   DOI={10.1007/978-3-031-43999-5_43},
   booktitle={Medical Image Computing and Computer Assisted Intervention – MICCAI 2023},
   publisher={Springer Nature Switzerland},
   author={Dorent, Reuben and Haouchine, Nazim and Kogl, Fryderyk and Joutard, Samuel and Juvekar, Parikshit and Torio, Erickson and Golby, Alexandra J. and Ourselin, Sebastien and Frisken, Sarah and Vercauteren, Tom and Kapur, Tina and Wells, William M.},
   year={2023},
   pages={448–458} }

@article{chadebec_DA,

   author={Chadebec, Clément and Thibeau-Sutre, Elina and Burgos, Ninon and Allassonnière, Stéphanie},
 
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
 
   title={Data Augmentation in High Dimensional Low Sample Size Setting Using a Geometry-Based Variational Autoencoder}, 
 
   year={2023},
 
   volume={45},
 
   number={3},
 
   pages={2879-2896},
 
   keywords={Data models;Measurement;Training;Magnetic resonance imaging;Databases;Three-dimensional displays;Task analysis;Variational autoencoders;data augmentation;latent space modeling},
 
   doi={10.1109/TPAMI.2022.3185773}}
 
@misc{sejnova:2024,
      title={Benchmarking Multimodal Variational Autoencoders: CdSprites+ Dataset and Toolkit}, 
      author={Gabriela Sejnova and Michal Vavrecka and Karla Stepanova and Tadahiro Taniguchi},
      year={2024},
      eprint={2209.03048},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.03048}, 
}

@misc{suzuki2023pixyz,
      title={Pixyz: a Python library for developing deep generative models}, 
      author={Masahiro Suzuki, Takaaki Kaneko and Yutaka Matsuo},
      journal = {Advanced Robotics},
      volume = {0},
      number = {0},
      pages = {1-16},
      year = {2023},
      publisher = {Taylor & Francis},
      doi = {10.1080/01691864.2023.2244568}
}